[learning]
nb_topics = 20
collection_passes = 10
document_passes = 1

[information]
# defaults to 0 if not set
background-topics-percentage = 0.2

# defaults to 1 if not set
default-class-weight = 1

# defaults to 0 if not set
ideology-class-weight = 10

[regularizers]
smooth-phi = smp
smooth-theta = smt
sparse-phi =
sparse-theta =

# Smooth Psi distributions
smooth-phi-dom-cls = sp-dc
smooth-phi-bac-cls =
smooth-phi-cls =

decorrelate-phi-dom-def = d_p-dd
label-regularization-phi-dom-cls = lrpdc

label-regularization-phi-dom-all =

label-regularization-phi =
decorrelate-phi-background =
decorrelate-phi-def =
decorrelate-phi-class = 

improve-coherence-phi = 
kl-function-info =
smooth-ptdw =
specified-sparse-phi = 
topic-selection = 

# topic-kernel and background-token-ratio support 0 < thresholds < 1.
# threshold values should have a maximum of 2 decimal digits
[scores]
perplexity = per
sparsity-phi-@dc = sppd
sparsity-phi-@ic = sppi
sparsity-theta = spt

# topic-kernel definition supports a custom 'probability_mass_threshold' parameter. It is recommended to be set to 0.5 and higher, it should be set once and remain fixed.
# Only if p(topic|word) > probability_mass_threshold then a token is considered to be part of the kernel.

# topic-kernel-0.25 = tk25  # used in paper 08; See 'Additive Regularization of Topic Models for Topic Selection and Sparse Factorization.pdf

topic-kernel-0.60 = tk60
topic-kernel-0.80 = tk80
topic-kernel-0.90 = tk90

top-tokens-10 = tt10
top-tokens-100 = tt100

# Computes KL - divergence between p(topic) and p(topic|word) distributions \mathrm{KL}(p(t) | | p(t | w)) (or vice versa)
# for each token and counts the part of tokens that have this value greater than the given (non-negative) delta_threshold.
# Such tokens are considered to be background ones.
background-tokens-ratio-0.3 = btr0
background-tokens-ratio-0.2 = btr1

items-processed =
topic-mass-phi =
theta-snippet =
