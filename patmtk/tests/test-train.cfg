[learning]
nb_topics = 20
collection_passes = 100
document_passes = 1

[information]
# defaults to 0 if not set
background-topics-percentage = 0.2

# defaults to 1 if not set
default-class-weight = 1

# defaults to 0 if not set
ideology-class-weight = 10

[regularizers]
sparse-phi = 
smooth-phi = smp
sparse-theta = 
smooth-theta = smt

# Smooth Psi distributions
smooth-phi-dom-cls = sp-dc
smooth-phi-bac-cls =
smooth-phi-cls = 

label-regularization-phi-dom-all = 
label-regularization-phi-dom-cls = 
label-regularization-phi = 
decorrelate-phi-dom-def = d_p-dd
decorrelate-phi-background = 
decorrelate-phi-def =
decorrelate-phi-class = 

improve-coherence-phi = 
kl-function-info =
smooth-ptdw =
specified-sparse-phi = 
topic-selection = 

# topic-kernel and background-token-ratio support 0 < thresholds < 1.
# threshold values should have a maximum of 2 decimal digits
[scores]
perplexity = per
sparsity-phi-@dc = sppd
sparsity-phi-@ic = sppi
sparsity-theta = spt

# topic-kernel definition supports a custom 'probability_mass_threshold' parameter. It is recommended to be set to 0.5 and higher, it should be set once and remain fixed.
# Only if p(topic|word) > probability_mass_threshold then a token is considered to be part of the kernel.

# topic-kernel-0.25 = tk25  # used in paper 08; See 'Additive Regularization of Topic Models for Topic Selection and Sparse Factorization.pdf

topic-kernel-0.60 = tk60
topic-kernel-0.80 = tk80
topic-kernel-0.90 = tk90

top-tokens-10 = tt10
top-tokens-100 = tt100

# Computes KL - divergence between p(topic) and p(topic|word) distributions \mathrm{KL}(p(t) | | p(t | w)) (or vice versa)
# for each token and counts the part of tokens that have this value greater than the given (non-negative) delta_threshold.
# Such tokens are considered to be background ones.
background-tokens-ratio-0.3 = btr0
background-tokens-ratio-0.2 = btr1

items-processed =
topic-mass-phi =
theta-snippet =
